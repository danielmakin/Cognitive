{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# importing of modules for LeNet CNN\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.optimizers.legacy import SGD, RMSprop, Adam\n",
    "\n",
    "# importing of service libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet class defined.\n"
     ]
    }
   ],
   "source": [
    "class LeNet:\n",
    "\t@staticmethod\n",
    "\tdef build(input_shape, classes):\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tmodel.add(Conv2D(32, kernel_size=3, padding=\"same\", input_shape=input_shape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tmodel.add(Conv2D(50, kernel_size=3, padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "\t\t# Flatten => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(500))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\n",
    "\t\t# a softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\treturn model\n",
    "\n",
    "print('LeNet class defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded\n"
     ]
    }
   ],
   "source": [
    "# Data Downloading\n",
    "\n",
    "# Read all of the Necessary Images and add Labels\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# Load the iCub Dataset\n",
    "def load_images_from_folder():\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, f in enumerate(['book', 'cellphone', 'hairbrush', 'hairclip', 'mouse', 'pencilcase', 'perfume', 'ringbinder', 'sunglasses', 'wallet']):\n",
    "        # Iterate through all files in the folder\n",
    "        for filename in os.listdir(os.path.join('selected_images/', f)):\n",
    "            # Check if the file is an image (you may want to add more file extensions)\n",
    "            if filename.endswith((\".jpg\")):\n",
    "                # Build the full path of the image file\n",
    "                file_path = os.path.join(os.path.join('selected_images/', f), filename)\n",
    "                # Open the image file\n",
    "                img = Image.open(file_path)\n",
    "                # Downsize the Images\n",
    "                img = img.resize((32, 32))\n",
    "                # Convert the image to a NumPy array\n",
    "                img_array = np.array(img)\n",
    "                X.append(img_array)\n",
    "                y.append(i)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = load_images_from_folder()\n",
    "print(\"Data Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_X_train shape: (21362, 32, 32, 3)\n",
      "21362 train samples\n",
      "5341 test samples\n"
     ]
    }
   ],
   "source": [
    "# Data Split 80:20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the Seperate Sets of Data\n",
    "input_X_train, input_X_test, output_y_train, output_y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('input_X_train shape:', input_X_train.shape)\n",
    "print(input_X_train.shape[0], 'train samples')\n",
    "print(input_X_test.shape[0], 'test samples')\n",
    " \n",
    "# Tell the model there are 10 classes\n",
    "output_Y_train = utils.to_categorical(output_y_train, 10)\n",
    "output_Y_test = utils.to_categorical(output_y_test, 10) \n",
    "\n",
    "# float and normalization\n",
    "input_X_train = input_X_train.astype('float32')\n",
    "input_X_test = input_X_test.astype('float32')\n",
    "input_X_train /= 255\n",
    "input_X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main variables initialised.\n"
     ]
    }
   ],
   "source": [
    "# seed for reproducibility\n",
    "np.random.seed(1671)\n",
    "\n",
    "# network and training parameters\n",
    "N_EPOCH = 20 # later use 20 for better results\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "IMG_ROWS, IMG_COLS = 32, 32 # input  dimensions of each MNIST image\n",
    "N_CLASSES = 10  # number of outputs = number of digits\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\n",
    "\n",
    "print('Main variables initialised.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 50)        14450     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 16, 16, 50)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 50)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               1600500   \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1620280 (6.18 MB)\n",
      "Trainable params: 1620280 (6.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer and compile the model\n",
    "model = LeNet.build(input_shape=INPUT_SHAPE, classes=N_CLASSES)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_1' (type Sequential).\n    \n    Input 0 of layer \"conv2d_2\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 32, 32, 3)\n    \n    Call arguments received by layer 'sequential_1' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVALIDATION_SPLIT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/9h/tqk_65w50d95gtyf68q0hv880000gn/T/__autograph_generated_file7gi8c70t.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/danielmakin/Library/Python/3.9/lib/python/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_1' (type Sequential).\n    \n    Input 0 of layer \"conv2d_2\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (None, 32, 32, 3)\n    \n    Call arguments received by layer 'sequential_1' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 32, 32, 3), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_X_train, output_y_train, batch_size=BATCH_SIZE, epochs=N_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
